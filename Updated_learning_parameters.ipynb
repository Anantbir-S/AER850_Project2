{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrMFbel8qUgowf+4MoqhlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anantbir-S/AER850_Project2/blob/main/Updated_learning_parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YjqrEZSEdIi",
        "outputId": "2b032403-879e-4c68-9e70-9c36aa9d26db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 1942 images belonging to 3 classes.\n",
            "Found 431 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 3/61\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36:07\u001b[0m 37s/step - accuracy: 0.4028 - loss: 159.0625"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define image dimensions\n",
        "input_image_shape = (500, 500, 3)\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/AER850 Project 2/Project 2 Data/Data'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'valid')\n",
        "\n",
        "# Enhanced data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for validation\n",
        "\n",
        "# Create image data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(500, 500),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(500, 500),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define the CNN model with an additional convolutional layer\n",
        "model = Sequential([\n",
        "    Input(shape=input_image_shape),\n",
        "    Conv2D(16, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(alpha=0.2),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    Conv2D(64, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(negative_slope=0.2),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    Conv2D(128, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(negative_slope=0.2),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Fourth Convolutional Block (Additional Layer for More Complexity)\n",
        "    Conv2D(256, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    LeakyReLU(negative_slope=0.2),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.6),  # Higher dropout for further regularization\n",
        "\n",
        "    # Output Layer\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Compile the model with an initial learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=20,  # Increased epochs for more training time\n",
        "    callbacks=[lr_scheduler, early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Plot training and validation accuracy/loss\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}